Args in experiment:
Namespace(is_training=1, train_only=True, model_id='512to1512_Autoformer', model='Autoformer', data='custom', root_path='/home/sgh/yes/envs/DACON/(4rd)Bitcoin_price_tsf/', data_path='train.csv', features='M', target='Close', freq='h', checkpoints='./checkpoints/', seq_len=512, label_len=256, pred_len=1512, individual=False, embed_type=0, enc_in=9, dec_in=9, c_out=9, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=0, num_workers=10, itr=1, train_epochs=1, batch_size=16, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : 512to1512_Autoformer_Autoformer_custom_ftM_sl512_ll256_pl1512_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36178
	iters: 100, epoch: 1 | loss: 0.5453318
	speed: 0.2241s/iter; left time: 484.4477s
	iters: 200, epoch: 1 | loss: 0.4553140
	speed: 0.2079s/iter; left time: 428.6275s
	iters: 300, epoch: 1 | loss: 0.4437062
	speed: 0.2080s/iter; left time: 408.1162s
	iters: 400, epoch: 1 | loss: 0.5374274
	speed: 0.2081s/iter; left time: 387.4407s
	iters: 500, epoch: 1 | loss: 0.6151399
	speed: 0.2082s/iter; left time: 366.8148s
	iters: 600, epoch: 1 | loss: 0.3825210
	speed: 0.2082s/iter; left time: 345.9802s
	iters: 700, epoch: 1 | loss: 0.5196214
	speed: 0.2085s/iter; left time: 325.6333s
	iters: 800, epoch: 1 | loss: 0.6407310
	speed: 0.2086s/iter; left time: 304.9373s
	iters: 900, epoch: 1 | loss: 0.5640175
	speed: 0.2086s/iter; left time: 284.1296s
	iters: 1000, epoch: 1 | loss: 0.7896090
	speed: 0.2086s/iter; left time: 263.2524s
	iters: 1100, epoch: 1 | loss: 0.5512725
	speed: 0.2084s/iter; left time: 242.2118s
	iters: 1200, epoch: 1 | loss: 0.4814800
	speed: 0.2085s/iter; left time: 221.4064s
	iters: 1300, epoch: 1 | loss: 0.3599499
	speed: 0.2085s/iter; left time: 200.6210s
	iters: 1400, epoch: 1 | loss: 0.5812733
	speed: 0.2086s/iter; left time: 179.7701s
	iters: 1500, epoch: 1 | loss: 0.3534762
	speed: 0.2085s/iter; left time: 158.8935s
	iters: 1600, epoch: 1 | loss: 0.4120347
	speed: 0.2085s/iter; left time: 138.0120s
	iters: 1700, epoch: 1 | loss: 0.4166855
	speed: 0.2085s/iter; left time: 117.1837s
	iters: 1800, epoch: 1 | loss: 0.6856419
	speed: 0.2085s/iter; left time: 96.3229s
	iters: 1900, epoch: 1 | loss: 0.4885260
	speed: 0.2084s/iter; left time: 75.4519s
	iters: 2000, epoch: 1 | loss: 0.5359659
	speed: 0.2084s/iter; left time: 54.6064s
	iters: 2100, epoch: 1 | loss: 0.3351220
	speed: 0.2085s/iter; left time: 33.7701s
	iters: 2200, epoch: 1 | loss: 0.3125735
	speed: 0.2084s/iter; left time: 12.9223s
Epoch: 1 cost time: 472.90540528297424
Epoch: 1, Steps: 2261 | Train Loss: 0.4569669
Validation loss decreased (inf --> 0.456967).  Saving model ...
Updating learning rate to 0.0001
